{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec3efa2",
   "metadata": {},
   "source": [
    "# Adaptive Fourier Neural Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bb444",
   "metadata": {},
   "source": [
    "In this notebook we will introduce the brief theory behind the Adaptive Fourier Neural Operators and use them to solve the same data-driven Darcy flow problem that was introduced in the [FNO notebook](Darcy_Flow_using_Fourier_Neural_Operators.ipynb)\n",
    "\n",
    "In contrast with the Fourier Neural Operator which has a convolutional architecture, the AFNO leverages contemporary transformer architectures in the computer vision domain. Vision transformers have delivered tremendous success in computer vision. This is primarily due to effective self-attention mechanisms. To cope with this challenge, Guibas et al. proposed [Adaptive Fourier Neural Operator (AFNO)](https://www.researchgate.net/publication/356601975_Adaptive_Fourier_Neural_Operators_Efficient_Token_Mixers_for_Transformers) as an efficient attention mechanism in the Fourier Domain. AFNO is based on principled foundation of operator learning which allows us to frame attention as a continuous global convolution efficiently in the Fourier domain. To handle challenges in vision such as discontinuities in images and high resolution inputs, AFNO proposes principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. \n",
    "\n",
    "This notebook uses the AFNO transformer for modeling a PDE system. While AFNO has been designed for scaling to extremely high resolution inputs that the FNO cannot handle as well ([FourCastNet](https://arxiv.org/pdf/2202.11214.pdf)), here we present a simple example using Darcy flow. This problem is intended as an illustrative starting point for data-driven training using AFNO in Modulus, but should not be regarded as leveraging the full extent of AFNO's functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2f7c9",
   "metadata": {},
   "source": [
    "### Learning Outcomes\n",
    "\n",
    "1. How to use the AFNO transformer architecture in Modulus\n",
    "2. Differences between the AFNO transformer and the FNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ca8ce",
   "metadata": {},
   "source": [
    "## AFNO Theory\n",
    "\n",
    "The Adaptive-Fourier Neural Operator (AFNO) architecture is highly effective and computationally efficient for high-resolution inputs. It combines a key recent advance in modeling PDE systems, namely the Fourier Neural Operator (FNO) with the powerful Vision Transformer (ViT) model for image processing. FNO has shown great results in modeling PDE systems such as Navier-Stokes flows. The ViT and related variants of transformer models have achieved SOTA performance in image processing tasks. The multi-head self-attention (MHSA) mechanism of the ViT is key to its impressive performance. The self-attention mechanism models long-range interactions at each layer of the neural network, a feature that is absent in most convolutional neural networks. The drawback of the ViT self-attention architecture is that it scales as a quadratic function of the length of the token sequence, and thus scales quadratically with input image resolution. The AFNO provides a solution to the scaling complexity of the ViT. The AFNO model implements a token mixing operation in the Fourier Domain. The computational complexity of the mixing operation is $\\mathcal{O}(N_{token}\\log N_{token})$ as opposed to the $\\mathcal{O}({N_{token}^2})$ complexity of the vanilla ViT architecture.\n",
    "\n",
    "The first step in the architecture involves dividing the input image into a regular grid with $h \\times w$ equal sized patches of size $p\\times p$. The parameter $p$ is referred to as the patch size. For simplicity, we consider a single channel image. Each patch is embedded into a token of size $d$, the embedding dimension. The patch embedding operation results in a token tensor ($X_{h\\times w \\times d}$) of size $h \\times w \\times d$. The patch size and embedding dimension are user-selected parameters. A smaller patch size allows the model to capture fine-scale details better while increasing the computational cost of training the model. A higher embedding dimension also increases the parameter count of the model. The token tensor is then processed by multiple layers of the transformer architecture performing spatial and channel mixing. \n",
    "\n",
    "The AFNO architecture implements the following operations in each layer.\n",
    "\n",
    "The token tensor is first transformed to the Fourier domain with\n",
    "\n",
    "\\begin{equation}\n",
    "z_{m,n} = [\\mathrm{DFT}(X)]_{m,n},\n",
    "\\end{equation}\n",
    "\n",
    "where $m,n$ is the index the patch location and DFT denotes a 2D discrete Fourier transform. The model then applies token weighting in the Fourier domain and promotes sparsity with a Soft-Thresholding and Shrinkage operation as\n",
    "\n",
    "\\begin{equation} \n",
    "\\tilde{z}_{m,n} = S_{\\lambda} ( \\mathrm{MLP}(z_{m,n})),\n",
    "\\end{equation}\n",
    "\n",
    "where $S_{\\lambda}(x) = \\mathrm{sign}(x) \\max(|x| - \\lambda, 0)$ with the sparsity controlling parameter $\\lambda$, and $\\mathrm{MLP(\\cdot)}$ is a 2-layer multi-layer perceptron with block-diagonal weight matrices which are shared across all patches. The number of blocks in the block diagonal MLP weight matrices is a user-selected hyperparameter that should be tuned appropriately. The last operation in a ANFO layer is an inverse Fourier to transform back to the patch domain and add a residual connection as\n",
    "\n",
    "\\begin{equation}\n",
    "y_{m,n} = [\\mathrm{IDFT}(\\tilde{Z})]_{m,n} + X_{m,n}.\n",
    "\\end{equation}\n",
    "\n",
    "At the end of all the transformer layers, a linear decoder converts the feature tensor back to the image space.\n",
    "\n",
    "There are several important hyper-parameters that affect the accuracy and computational cost of the AFNO. Empirically, the most important hyperparameters that should be tuned keeping in mind the task at hand are the number of layers, patch size, the embedding dimension and the number of blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023903f",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The problem description follows from the FNO notebook. We would be building a surrogate model that learns the mapping between a permeability and pressure field of a Darcy flow system. The AFNO is based on image transformer backbone and as with all transformer architectures, the AFNO tokenizes the input field. Each token is embedded from a patch of the image. The tokenized image is processed by the transformer layers followed by a linear decoder which generates the output image. \n",
    "\n",
    "<img src=\"afno_darcy.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792da78",
   "metadata": {},
   "source": [
    "## Case setup\n",
    "\n",
    "Similar to the FNO chapter, the training and validation data for this example can be found on the [Fourier Neural Operator Github page](https://github.com/zongyi-li/fourier_neural_operator). The data can be downloaded using an automated script similar to the FNO notebook. \n",
    "\n",
    "**Note:** In this notebook we will walk through the contents of [`darcy_AFNO_lazy.py`](../../source_code/darcy/darcy_AFNO.py) script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a44b4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The AFNO is based on the ViT transformer architecture and requires tokenization of the inputs. Here each token is a patch of the image with a patch size defined in the configuration file through the parameter `patch_size`. The `embed_dim` parameter defines the size of the latent embedded features used inside the model for each patch. The contents of the [`config_AFNO.yaml`](../../source_code/darcy/conf/config_AFNO.yaml) are shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9726657",
   "metadata": {},
   "source": [
    "```yaml\n",
    "defaults :\n",
    "  - modulus_default\n",
    "  - arch:\n",
    "      - afno\n",
    "  - scheduler: tf_exponential_lr\n",
    "  - optimizer: adam\n",
    "  - loss: sum\n",
    "  - _self_\n",
    "\n",
    "arch:\n",
    "  afno:\n",
    "    patch_size: 16\n",
    "    embed_dim: 256\n",
    "    depth: 4\n",
    "    num_blocks: 8\n",
    "    \n",
    "scheduler:\n",
    "  decay_rate: 0.95\n",
    "  decay_steps: 1000\n",
    "\n",
    "training:\n",
    "  rec_results_freq : 1000\n",
    "  max_steps : 10000\n",
    "\n",
    "batch_size:\n",
    "  grid: 32\n",
    "  validation: 32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ebb62",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Loading both the training and validation datasets into memory follows a similar process as seen in the FNO notebook. We will use the eager data loading for both the datasets in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c3146",
   "metadata": {},
   "source": [
    "```python\n",
    "import modulus\n",
    "from modulus.hydra import instantiate_arch\n",
    "from modulus.hydra.config import ModulusConfig\n",
    "from modulus.key import Key\n",
    "\n",
    "from modulus.domain import Domain\n",
    "from modulus.domain.constraint import SupervisedGridConstraint\n",
    "from modulus.domain.validator import GridValidator\n",
    "from modulus.dataset import DictGridDataset\n",
    "from modulus.solver import Solver\n",
    "\n",
    "from modulus.utils.io.plotter import GridValidatorPlotter\n",
    "\n",
    "from utilities import download_FNO_dataset, load_FNO_dataset\n",
    "\n",
    "\n",
    "@modulus.main(config_path=\"conf\", config_name=\"config_AFNO\")\n",
    "def run(cfg: ModulusConfig) -> None:\n",
    "\n",
    "    # load training/ test data\n",
    "    input_keys = [Key(\"coeff\", scale=(7.48360e00, 4.49996e00))]\n",
    "    output_keys = [Key(\"sol\", scale=(5.74634e-03, 3.88433e-03))]\n",
    "\n",
    "    download_FNO_dataset(\"Darcy_241\", outdir=\"datasets/\")\n",
    "    invar_train, outvar_train = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=1000,\n",
    "    )\n",
    "    invar_test, outvar_test = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=100,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924afdff",
   "metadata": {},
   "source": [
    "The inputs for AFNO need to be perfectly divisible by the specified patch size (in this case, `patch_size=16`), which is not the case for this dataset. Therefore, we trim the input/output features such that they are have appropriate dimensionality `241x241 -> 240x240`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abb772",
   "metadata": {},
   "source": [
    "```python\n",
    "    # get training image shape\n",
    "    img_shape = [\n",
    "        next(iter(invar_train.values())).shape[-2],\n",
    "        next(iter(invar_train.values())).shape[-1],\n",
    "    ]\n",
    "\n",
    "    # crop out some pixels so that img_shape is divisible by patch_size of AFNO\n",
    "    img_shape = [s - s % cfg.arch.afno.patch_size for s in img_shape]\n",
    "    print(f\"cropped img_shape: {img_shape}\")\n",
    "    for d in (invar_train, outvar_train, invar_test, outvar_test):\n",
    "        for k in d:\n",
    "            d[k] = d[k][:, :, : img_shape[0], : img_shape[1]]\n",
    "            print(f\"{k}: {d[k].shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7a7d4",
   "metadata": {},
   "source": [
    "## Initializing the Model\n",
    "\n",
    "Initializing the model and domain again follow the same steps as seen in the FNO notebook. For AFNO, we calculate the size of the domain after loading the dataset. The domain needs to be defined in the AFNO model, which is provided with the inclusion of the keyword argument `img_shape` in the `instantiate_arch` call. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc70c3b",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make list of nodes to unroll graph on\n",
    "    model = instantiate_arch(\n",
    "        input_keys=input_keys,\n",
    "        output_keys=output_keys,\n",
    "        cfg=cfg.arch.afno,\n",
    "        img_shape=img_shape,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285201b1",
   "metadata": {},
   "source": [
    "## Adding Data Constraints and Validators\n",
    "\n",
    "The data-driven constraints and validators are then added to the domain in the same fashion as the FNO notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3778ba9",
   "metadata": {},
   "source": [
    "```python\n",
    "    # add constraints to domain\n",
    "    supervised = SupervisedGridConstraint(\n",
    "        nodes=nodes,\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.batch_size.grid,\n",
    "    )\n",
    "    domain.add_constraint(supervised, \"supervised\")\n",
    "\n",
    "    # add validator\n",
    "    val = GridValidator(\n",
    "        nodes,\n",
    "        dataset=test_dataset,\n",
    "        batch_size=cfg.batch_size.validation,\n",
    "        plotter=GridValidatorPlotter(n_examples=5),\n",
    "    )\n",
    "    domain.add_validator(val, \"test\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbd468",
   "metadata": {},
   "source": [
    "## Solver and Training the model\n",
    "\n",
    "Once the domain and the configuration is setup, the `Solver` can be defined and the training can be started as seen in earlier notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc17fc",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make solver\n",
    "    slv = Solver(cfg, domain)\n",
    "\n",
    "    # start solver\n",
    "    slv.solve()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RANK\"]=\"0\"\n",
    "os.environ[\"WORLD_SIZE\"]=\"1\"\n",
    "os.environ[\"MASTER_ADDR\"]=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c021c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../source_code/darcy/darcy_AFNO.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98026935",
   "metadata": {},
   "source": [
    "## Results and Post-processing\n",
    "\n",
    "The checkpoint directory is saved based on the results recording frequency specified in the `rec_results_freq` parameter of its derivatives. The network directory folder contains several plots of the different validation predictions, some of which are shown below. \n",
    "\n",
    "AFNO validation predictions. (Left to right) Input permeability, true pressure, predicted pressure, error.\n",
    "\n",
    "<img src=\"afno_darcy_pred1.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "<img src=\"afno_darcy_pred2.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "<img src=\"afno_darcy_pred3.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "It is important to recognize that AFNO's strengths lie in its ability to scale to a much larger model size and datasets than what is used in this notebook/example. While not illustrated here, this example demonstrates the fundamental implementation of data-driven training using the AFNO architecture in Modulus for you to extend to larger problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08e9f2",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
