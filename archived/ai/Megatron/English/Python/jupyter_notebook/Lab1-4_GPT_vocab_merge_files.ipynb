{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-dating",
   "metadata": {},
   "source": [
    "## GPT Tokenizer files\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "The goal of this lab is to examine the difference between BPE and GPTBPE Tokenizer.\n",
    "\n",
    "Later on, we will use the observations from this notebook to train a GPTBPE Tokenizer with our own raw text data.\n",
    "\n",
    "We will load and verify the GPTBPE Tokenizer and make sure the output tokens and token ids are as expected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-albany",
   "metadata": {},
   "source": [
    "Let's review the source code of [GPT2 tokenizer](https://huggingface.co/transformers/_modules/transformers/tokenization_gpt2.html)\n",
    "\n",
    "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "\n",
    "    \n",
    "\n",
    "         from transformers import GPT2Tokenizer\n",
    "         tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "         tokenizer(\" Hello world\")['input_ids']\n",
    "        [18435, 995]\n",
    "\n",
    "We expect our custom tokenizer, which we will train later in lab 2,  will exhibit the same behavior of [treating spaces like parts of the tokens](https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2Tokenizer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-amount",
   "metadata": {},
   "source": [
    "Install necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-drinking",
   "metadata": {},
   "source": [
    "Next, we will proceed to fetch the pretrained GPT Tokenizer files, namely the vocab and merge files, will ideally looks like. \n",
    "\n",
    "We can later on use these observations to validate our custom trained GPTBPE tokenizer and the corresponding vocab.json and merges.txt file, in order to ensure the custom trained GPTBPE tokenizer will tokenze as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-sweet",
   "metadata": {},
   "source": [
    "Examine the vocab and merge files, observe the presence of Ġ character.\n",
    "Ġ = space + 256, this character is used as a control letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "with open('gpt2-vocab.json') as ip_file:\n",
    "    o = json.load(ip_file)\n",
    "    take=20\n",
    "    rn=random.randint(0,len(o)-1)\n",
    "    print(\"noted that the Ġ = space + 256 is the control letter\")\n",
    "    print(list(o.keys())[rn:rn+take])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 5 gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-tsunami",
   "metadata": {},
   "source": [
    "The following code block will load a default GPT2Tokenizer from HuggingFace's **_transformer_** library, verify the following:\n",
    "\n",
    "            from transformers import GPT2Tokenizer\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "            tokenizer(\" Hello world\")['input_ids']\n",
    "            expected token ids for \" Hello world\" is [18435, 995]\n",
    "\n",
    "Note: The HuggingFace's **transformer** library does not have functions to train the GPTBPE tokenizer, but it can load a pre-trained tokenizer given valid files. For training the GPTBPE Tokenizer, we will need to use another library called **tokenizers**, also from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print('\\n notice the **SPACE** in front of ** Hello world** \\n')\n",
    "sample_text=\" Hello world\"\n",
    "print(sample_text)\n",
    "out=tokenizer.tokenize(sample_text)\n",
    "print(\"tokens:\",out)\n",
    "ids=tokenizer(sample_text)['input_ids']\n",
    "print(\"ids:\",ids)\n",
    "## expected output :\n",
    "## [18435, 995]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-thousand",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "    \n",
    "         Hello world\n",
    "        tokens: ['ĠHello', 'Ġworld']\n",
    "        ids: [18435, 995]\n",
    "Observe the presence of the **Ġ** character. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-durham",
   "metadata": {},
   "source": [
    "In the next code block, we will examine how HuggingFace's **_tokenizers_** library loads a pretrained tokenizer given gpt2-vocab.json and merges.txt files. \n",
    "We will verify that, the usage of the `use_gpt` flag will result in the same tokenization behavior, i.e the presence of the **Ġ** character. We will also double check that the token IDs are identical to HuggingFace's **_transformer_** loaded `tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")` when applying tokenization to the exact same text ` Hello world`. \n",
    "\n",
    "Setting `use_gpt` to True will evoke the following : \n",
    "\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "        \n",
    "This is the expected tokenizer behavior for GPTBPE Tokenizer. This GPTBPE tokenizer will load the vocab.json and merges.txt files and tokenize as expected. Whereas setting the `use_gpt` flag to False, will result in a normal BPE Tokenizer and the tokenization will behave differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "import json\n",
    "\n",
    "def load_tokenizer(vocab_file,merge_file, use_gpt):\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.model = BPE.from_file(vocab_file, merge_file)\n",
    "    with open(vocab_file, 'r') as f2:\n",
    "        vocab = json.loads(f2.read())  \n",
    "    if use_gpt:\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "    return tokenizer , vocab\n",
    "vocab_file='./gpt2-vocab.json'\n",
    "merge_file='./gpt2-merges.txt'\n",
    "tokenizers_gpt,_=load_tokenizer(vocab_file,merge_file,True)\n",
    "sample_text=' Hello world' \n",
    "output=tokenizers_gpt.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "#print(tokens ,'\\n')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)\n",
    "\n",
    "tokenizers_bpe,_=load_tokenizer(vocab_file,merge_file, False)\n",
    "sample_text=' Hello world'\n",
    "output=tokenizers_bpe.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "print(\"---\"*10)\n",
    "print('\\nnotice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-executive",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "        tokens:  ['ĠHello', 'Ġworld']\n",
    "        ids: [18435, 995]\n",
    "        ------------------------------\n",
    "\n",
    "        notice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer\n",
    "        tokens:  ['H', 'ellow', 'orld']\n",
    "        ids: [39, 5037, 1764]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-equality",
   "metadata": {},
   "source": [
    "What did we observed? \n",
    "\n",
    "By setting the `use_gpt` flag to True in HuggingFace's **_tokenizers_** library when loading the same gpt2-vocab.json and merges.txt gives us the expected behavior of GPTBPE tokenization. \n",
    "\n",
    "We can further verify that by applying tokenization to the exact same text ` Hello world`, the result of the tokenizer, with the`use_gpt` flag = True, will match the result of the HuggingFace's  **_transformer_** library loaded gpt2 tokenizer.\n",
    "\n",
    "Whereas setting the `use_gpt` flag = False would result in a different behavior. \n",
    "\n",
    "Therefore, we will enforce having :\n",
    "\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "When training our own GPTBPETokenizer with our own raw text data in Lab 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-indie",
   "metadata": {},
   "source": [
    "We will now move the gpt-vocab.json and gpt2-merges.txt to the correct data folder as a preparation for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv gpt2-vocab.json ../dataset/EN/50k/\n",
    "!mv gpt2-merges.txt ../dataset/EN/50k/\n",
    "!ls ../dataset/EN/50k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-finland",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Links and Resources\n",
    "Don't forget to check out additional resources such as [HuggingFace Tokenizer Documentation](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) and [Train GPT-2 in your own language](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-technology",
   "metadata": {},
   "source": [
    "-----\n",
    "## <p style=\"text-align:center;border:3px; padding: 1em\"> <a href=../Start_Here.ipynb>HOME</a>&nbsp; &nbsp; &nbsp; <a href=./Lab1-5_jsonfy_and_process2mmap.ipynb>NEXT</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-preliminary",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
